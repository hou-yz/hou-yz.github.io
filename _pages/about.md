---
permalink: /
title: ""
excerpt: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

About
======
Yunzhong Hou (侯云钟) is a research fellow at the Australian National University, collaborating closely with [Prof. Tom Gedeon](https://staffportal.curtin.edu.au/staff/profile/view/tom-gedeon-5e48a1fd/#top) and [Prof. Liang Zheng](http://zheng-lab.cecs.anu.edu.au/). Prior to that, he completed his PhD at the same university in 2023, under the esteemed guidance of [Prof. Liang Zheng](http://zheng-lab.cecs.anu.edu.au/), [Prof. Stephen Gould](http://users.cecs.anu.edu.au/~sgould/), and [Prof. Hongdong Li](http://users.cecs.anu.edu.au/~hongdong/). He earned a bachelor's degree in electronic engineering from Tsinghua University in 2018.

Specializing in computer vision and deep learning, his current research interest lies in AI videography and photography, embodied agents, and 3D understanding. Yunzhong is actively involved in the academic community, contributing as a conference reviewer for prominent conferences such as CVPR, ECCV, ICCV, NeurIPS, ACM MM, and IJCAI. He also serves as a journal reviewer for respected publications like TPAMI, TIP, TMM, and TCSVT, and an area chair for ICASSP and ACM MM.


For more details, please find his CV [here](https://1drv.ms/b/s!AtzsQybTubHfhI8szRMzVSKg6dYy-Q?e=sFSths).

News
======

**2024.10** &emsp; I am serving as an Area Chair for ACM Multimedia 2024 [[full program]](https://2024.acmmm.org/files/MM24-Full_Program.pdf). An honor to serve as Session Chair and host Oral Session 13 - Machine Learning for Multimedia with [Prof. Chang Xu](http://changxu.xyz/). Excited to present our latest work on AI drone videography at the [ACM MM Area Chair Workshop](https://sites.google.com/view/mm24acworkshop/) for ACM MM 2024.

**2024.07** &emsp; Glad to present our latest work on AI drone videography at the International Research Workshop Data Science and AI & Robotics (DSAIR24) at University of Canberra following the invitation from [Prof. Shuangzhe Liu](https://researchprofiles.canberra.edu.au/en/persons/shuangzhe-liu).

**2024.05** &emsp; Our paper on color quantization and pixel art creation, _"[Scalable Deep Color Quantization: A Cluster Imitation Approach](https://ieeexplore.ieee.org/abstract/document/10596087)"_, is accepted by *IEEE Trans on Image Processing*. [paper](https://ieeexplore.ieee.org/abstract/document/10596087), [code](https://github.com/hou-yz/color_distillation_clustering).

**2024.02** &emsp; Our paper on camera configuration optimizaiton, _"[Learning to Select Views for Efficient Multi-View Understanding](https://arxiv.org/abs/2303.06145)"_, is accepted by CVPR 2024. See you in Seattle! [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Hou_Learning_to_Select_Views_for_Efficient_Multi-View_Understanding_CVPR_2024_paper.pdf), [code](https://github.com/hou-yz/MVSelect).



**2023.12** &emsp; Our grant proposal _"Privacy-Percerving Perception for Robotics"_ is awarded by the [HMI seed grant](https://services.anu.edu.au/research-support/funding-opportunities/computing-for-social-good-seed-grants-2023) for 25,000 AUD! Big thank you to [Dylan](https://sites.google.com/view/djcampbell), [Rahul](https://rahulsho.me/), and [Mike](https://michaelrandallbarnes.com/about)!

**2023.12** &emsp; Check out our latest research on camera layout optimization _"[Optimizing Camera Configurations for Multi-View Pedestrian Detection](https://arxiv.org/abs/2312.02144)"_. [arxiv](https://arxiv.org/abs/2312.02144)

**2023.11** &emsp; Our paper _"[View-Coherent Correlation Consistency for Semi-Supervised Semantic Segmentation](https://www.sciencedirect.com/science/article/pii/S0031320323007860)"_ is accepted by *Pattern recognition*. [paper](https://www.sciencedirect.com/science/article/pii/S0031320323007860).

**2023.06** &emsp;   I was named outstanding reviewer for CVPR 2023!

**2023.04** &emsp;   Joined as a research fellow at ANU, working with [Prof. Tom Gedeon](https://staffportal.curtin.edu.au/staff/profile/view/tom-gedeon-5e48a1fd/#top) and [Dr. Liang Zheng](http://zheng-lab.cecs.anu.edu.au/). Excited!

**2023.03** &emsp;   Check out our latest research _"[Learning to Select Camera Views: Efficient Multiview Understanding at Few Glances](https://arxiv.org/abs/2303.06145)"_ on arXiv. [paper](https://arxiv.org/abs/2303.06145), [code](https://github.com/hou-yz/MVSelect)

**2022.07** &emsp;   Internship at Amazon Web Services as a research scientist on vision-language tasks. Hello Bay Area!

**2021.12** &emsp;   Our paper _"[Adaptive Affinity for Associations in Multi-Target Multi-Camera Tracking](/publication/2022-tip-mtmc)"_ is accepted by *IEEE Trans on Image Processing*. [paper](https://ieeexplore.ieee.org/iel7/83/4358840/09646485.pdf), [code](https://github.com/hou-yz/DeepCC-local)

**2021.07** &emsp; Our paper _"[Ranking Models in Unlabeled New Environments](/publication/2021-iccv2021-ranking)"_ is accepted by *ICCV 2021*. [paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Sun_Ranking_Models_in_Unlabeled_New_Environments_ICCV_2021_paper.pdf), [code](https://github.com/sxzrt/Proxy-Set)

**2021.07** &emsp; Our paper _"[Multiview Detection with Shadow Transformer (and View-Coherent Data Augmentation)](/publication/2021-acmmm2021-mvdetr)"_ is accepted by *ACM MM 2021*. [paper](https://arxiv.org/abs/2108.05888), [code](https://github.com/hou-yz/MVDeTr)

**2021.03** &emsp; Our paper _"[Visualizing Adapted Knowledge in Domain Transfer](/publication/2020-cvpr2021-sfit)"_ is accepted by *CVPR 2021*.
[paper](https://arxiv.org/abs/2104.10602), [code](https://github.com/hou-yz/DA_visualization), [知乎-UDA可视化](https://zhuanlan.zhihu.com/p/369252839), [知乎-无需风格图像的风格迁移](https://zhuanlan.zhihu.com/p/371101640).

**2020.07** &emsp; Our paper _"[Multiview Detection with Feature Perspective Transformation](/publication/2020-eccv2020-mvdet)"_ is accepted by *ECCV 2020*. 
[paper](https://arxiv.org/abs/2007.07247), [code](https://github.com/hou-yz/MVDet), [知乎](https://zhuanlan.zhihu.com/p/196771711), [MultiviewX dataset download](https://1drv.ms/u/s!AtzsQybTubHfgP9BJt2g7R_Ku4X3Pg?e=GFGeVn).


**2020.03** &emsp; Our paper _"[Learning to Structure an Image with Few Colors](/publication/2019-cvpr2020-colorcnn)"_ is accepted by *CVPR 2020*. 
[paper](http://openaccess.thecvf.com/content_CVPR_2020/papers/Hou_Learning_to_Structure_an_Image_With_Few_Colors_CVPR_2020_paper.pdf), [code](https://github.com/hou-yz/color_distillation), [知乎](https://zhuanlan.zhihu.com/p/148160812).

**2019.11**  &emsp; A new paper "[Locality aware appearance metric for multi-target multi-camera tracking](/publication/2019-arxiv-mtmc-metric)" is released on *arXiv*. 
[paper](https://arxiv.org/abs/1911.12037.pdf), [code](https://github.com/hou-yz/DeepCC-local), [知乎](https://zhuanlan.zhihu.com/p/96999382).

**2019.06** &emsp; Won 5th place out of 22 participants in multi-target multi-camera tracking in *CVPR 2019 AI-City Challenge*. [paper](http://openaccess.thecvf.com/content_CVPRW_2019/papers/AI%20City/Hou_A_Locality_Aware_City-Scale_Multi-Camera_Vehicle_Tracking_System_CVPRW_2019_paper.pdf), [code](https://github.com/hou-yz/DeepCC-local).

**2019.06** &emsp; Won 3rd place out of 84 participants in vehicle re-identification in *CVPR 2019 AI-City Challenge*. [paper](http://openaccess.thecvf.com/content_CVPRW_2019/papers/AI%20City/Lv_Vehicle_Re-Identification_with_Location_and_Time_Stamps_CVPRW_2019_paper.pdf), [code](https://github.com/hou-yz/open-reid-tracking).

**2019.03**  &emsp; Our paper "[Improving Device-Edge Cooperative Inference of Deep Learning via 2-Step Pruning](/publication/2018-infocom2019workshop-pruning)" is accepted by *Infocom workshop on IECOO 2019*. [paper](https://arxiv.org/abs/1903.03472.pdf), [code](https://github.com/hou-yz/pytorch-pruning-2step).





Contact
------
Please contact me via [e-mail](mailto:yunzhong.hou@anu.edu.au).
